{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmb6kRF3NtJTgJa+0w39d6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragyasingh1729/DeepLearningProjects/blob/main/LSTM_wordNextPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Artificial Neural Network (ANN)\n",
        "\n",
        "So far, we were working with ANN which is a sophisticated computational model inspired by the workings of the human brain. It consists of layers of interconnected nodes, known as neurons, which work together to process and interpret complex data.\n",
        "\n",
        "Each neuron in the network functions as a mathematical unit that receives input, processes it, and passes the output to the next layer of neurons. This structure typically includes an input layer, one or more hidden layers, and an output layer.\n",
        "\n",
        "Through a process called training, where the weights of the connections between neurons are adjusted, ANNs can learn from data, recognize patterns, and make predictions. This capability makes them invaluable in a wide range of applications, such as image and speech recognition, natural language processing, and predictive analytics, enabling advancements in technology and artificial intelligence.\n",
        "\n",
        "\n",
        "## Recurrent Neural Network\n",
        "\n",
        "### Why do we need RNN?\n",
        "- Fixed number of input and output in ANN\n",
        "- What if we fix the size of input by padding? Then it leads to next problem that is ANN does not take input sequence into consideration\n",
        "\n",
        "### Simple understanding of RNN\n",
        "RNNs have a recurrent connection that allows them to maintain a memory of previous inputs, making them well-suited for tasks involving sequential data such as time series prediction, natural language processing, and speech recognition.\n",
        "\n",
        "Here's a simple understanding of Recurrent Neural Networks (RNNs). Imagine you're reading a story, and you need to understand each sentence based on what you've read so far. That's what an RNN does - it reads data one part at a time and remembers what it's seen before.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Taking Input**: Just like reading a story, an RNN takes data (like words or numbers) one step at a time.\n",
        "   \n",
        "2. **Remembering**: As it reads each part, it keeps a memory of what it's seen so far. This memory helps it understand the current part in the context of what came before.\n",
        "\n",
        "3. **Predicting**: Once it's read all the parts, it can make predictions or decisions based on what it's learned from the whole sequence.\n",
        "\n",
        "**Example:**\n",
        "Imagine you're predicting the next word in a sentence. With each word you read, the RNN updates its memory to understand what might come next. So if you've read \"The cat is\", it might predict \"sleeping\" as the next word because it knows \"The cat is\" often followed by \"sleeping\".\n",
        "\n",
        "**In Summary:**\n",
        "An RNN is like a smart reader that understands not just the current part of the story, but how it fits into the whole tale.\n",
        "\n",
        "\"Recurrent\" in RNNs highlights their ability to process sequential data by repeatedly applying the same computation across different time steps, incorporating information from previous steps into the current computation.\n",
        "\n",
        "### Different mapping\n",
        "In the world of sequences, how information moves from one point to another matters a lot. Let's break down three different input output mappping with an example\n",
        "- **one to many** - write a caption for an input image\n",
        "- **many to one** - take review of a movie as an input and give rating as output\n",
        "- **many to many** - language translation\n",
        "\n",
        "### Long Short Term Memory\n",
        "- **RNNs and Vanishing Gradient Problem**:\n",
        "  - RNNs (Recurrent Neural Networks) struggle with long sequential data due to the vanishing gradient problem, where gradients become too small, hindering learning over long time steps.\n",
        "\n",
        "- **LSTM Architecture**:\n",
        "  - LSTM (Long Short-Term Memory) networks address this issue with a unique architecture that maintains long-term and short-term memory.\n",
        "  - This is achieved using three gates that regulate the flow of information:\n",
        "\n",
        "    - **Forget Gate**: Decides what information to discard from the long-term state. It uses a sigmoid activation function to produce a value between 0 and 1, where 0 means \"completely forget\" and 1 means \"completely keep.\"\n",
        "  \n",
        "    - **Input Gate**: Determines what new information to add to the cell state. It consists of a sigmoid layer (deciding which values to update) and a tanh layer (creating new candidate values to add).\n",
        "  \n",
        "    - **Output Gate**: Updates the hidden state, which is used for predictions and to inform the next time step. It filters the cell state through a sigmoid function and multiplies it by the tanh of the cell state.\n",
        "\n",
        "- **Additional Points**:\n",
        "  - **Cell State**: The cell state runs through the entire chain with only minor linear interactions, allowing the network to carry information across many time steps without significant loss.\n",
        "  - **Hidden State**: The hidden state is updated at each time step and serves as a short-term memory that influences the cell state and the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "tL-64o6q3KVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\"><b>Working with LSTM</b></font>\n",
        "\n"
      ],
      "metadata": {
        "id": "lZpFardt3Ulh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HLmFWFY6zUPU"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"About the Program\n",
        "What is the course fee for  Data Science Mentorship Program (DSMP 2023)\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Python Fundamentals\n",
        "Python libraries for Data Science\n",
        "Data Analysis\n",
        "SQL for Data Science\n",
        "Maths for Machine Learning\n",
        "ML Algorithms\n",
        "Practical ML\n",
        "MLOPs\n",
        "Case studies\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390\n",
        "Will Deep Learning and NLP be a part of this program?\n",
        "No, NLP and Deep Learning both are not a part of this program’s curriculum.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.\n",
        "Where can I find the class schedule?\n",
        "Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "What is the time duration of all the live sessions?\n",
        "Roughly, all the sessions last 2 hours.\n",
        "What is the language spoken by the instructor during the sessions?\n",
        "Hinglish\n",
        "How will I be informed about the upcoming class?\n",
        "You will get a mail from our side before every paid session once you become a paid user.\n",
        "Can I do this course if I am from a non-tech background?\n",
        "Yes, absolutely.\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "If I join/pay in the middle, will I be able to see all the past lectures?\n",
        "Yes, once you make the payment you will be able to see all the past content in your dashboard.\n",
        "Where do I have to submit the task?\n",
        "You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.\n",
        "Will we do case studies in the program?\n",
        "Yes.\n",
        "Where can we contact you?\n",
        "You can mail us at nitish.campusx@gmail.com\n",
        "Payment/Registration related questions\n",
        "Where do we have to make our payments? Your YouTube channel or website?\n",
        "You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/\n",
        "Can we pay the entire amount of Rs 5600 all at once?\n",
        "Unfortunately no, the program follows a monthly subscription model.\n",
        "What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb\n",
        "15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.\n",
        "What if I don’t like the course after making the payment. What is the refund policy?\n",
        "You get a 7 days refund period from the day you have made the payment.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmail.com\n",
        "Post registration queries\n",
        "Till when can I view the paid videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.\n",
        "But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.\n",
        "Why lifetime validity is not provided?\n",
        "Because of the low course fee.\n",
        "Where can I reach out in case of a doubt after the session?\n",
        "You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session\n",
        "If I join the program late, can I still ask past week doubts?\n",
        "Yes, just select past week doubt in the doubt clearance google form.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmai.com\n",
        "Certificate and Placement Assistance related queries\n",
        "What is the criteria to get the certificate?\n",
        "There are 2 criterias:\n",
        "You have to pay the entire fee of Rs 5600\n",
        "You have to attempt all the course assessments.\n",
        "I am joining late. How can I pay payment of the earlier months?\n",
        "You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.\n",
        "I have read that Placement assistance is a part of this program. What comes under Placement assistance?\n",
        "This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance\n",
        "Portfolio Building sessions\n",
        "Soft skill sessions\n",
        "Sessions with industry mentors\n",
        "Discussion on Job hunting strategies\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps to predict the next word using LSTM\n",
        "- Tokenizer: to convert the raw text into sequence to integers\n",
        "  - first tokenized all the words in the given input\n",
        "  - use the token to convert the sentence into list of token\n",
        "\n",
        "- now we want to go word-by-word and get the input and target (next word which will come in the sentence)\n",
        "\n",
        "- the list of input and output will vary in size, so use the padding to get the same length\n",
        "\n",
        "- overall the problem would be `classification problem` as the target would be the integer value associated to the word in the input directory\n",
        "\n",
        " ### Tokenizer\n",
        " It is designed to transform raw text into sequences of integers, which can then be fed into neural network models."
      ],
      "metadata": {
        "id": "mrbTz7gOA66l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "gBtq1DW04AUe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "p7ey0cFWAK4T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "7QFm4PQ0AWmm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_3QWjrpAYf4",
        "outputId": "28e70176-bed5-4401-a424-17bc4434136d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'you': 2,\n",
              " 'i': 3,\n",
              " 'to': 4,\n",
              " 'a': 5,\n",
              " 'of': 6,\n",
              " 'is': 7,\n",
              " 'have': 8,\n",
              " 'will': 9,\n",
              " 'can': 10,\n",
              " 'what': 11,\n",
              " 'course': 12,\n",
              " 'program': 13,\n",
              " 'in': 14,\n",
              " 'for': 15,\n",
              " 'all': 16,\n",
              " 'sessions': 17,\n",
              " 'on': 18,\n",
              " 'be': 19,\n",
              " 'and': 20,\n",
              " 'this': 21,\n",
              " 'if': 22,\n",
              " 'am': 23,\n",
              " 'pay': 24,\n",
              " 'payment': 25,\n",
              " 'make': 26,\n",
              " 'we': 27,\n",
              " 'do': 28,\n",
              " 'subscription': 29,\n",
              " 'where': 30,\n",
              " 'rs': 31,\n",
              " 'so': 32,\n",
              " 'campusx': 33,\n",
              " 'session': 34,\n",
              " 'our': 35,\n",
              " 'paid': 36,\n",
              " 'join': 37,\n",
              " 'able': 38,\n",
              " 'your': 39,\n",
              " 'website': 40,\n",
              " 'placement': 41,\n",
              " 'fee': 42,\n",
              " 'data': 43,\n",
              " 'monthly': 44,\n",
              " 'month': 45,\n",
              " 'not': 46,\n",
              " 'get': 47,\n",
              " 'yes': 48,\n",
              " 'once': 49,\n",
              " 'past': 50,\n",
              " 'feb': 51,\n",
              " 'assistance': 52,\n",
              " 'science': 53,\n",
              " '7': 54,\n",
              " '5600': 55,\n",
              " 'are': 56,\n",
              " 'watch': 57,\n",
              " 'google': 58,\n",
              " 'by': 59,\n",
              " 'com': 60,\n",
              " 'mail': 61,\n",
              " 'from': 62,\n",
              " 'contact': 63,\n",
              " 'us': 64,\n",
              " 'at': 65,\n",
              " 'or': 66,\n",
              " 'doubt': 67,\n",
              " 'mentorship': 68,\n",
              " 'payments': 69,\n",
              " '799': 70,\n",
              " 'total': 71,\n",
              " 'duration': 72,\n",
              " 'months': 73,\n",
              " 'learning': 74,\n",
              " 'case': 75,\n",
              " 'here': 76,\n",
              " 'https': 77,\n",
              " 'part': 78,\n",
              " 'see': 79,\n",
              " 'late': 80,\n",
              " 'dashboard': 81,\n",
              " 'task': 82,\n",
              " 'don’t': 83,\n",
              " 'nitish': 84,\n",
              " 'validity': 85,\n",
              " '15th': 86,\n",
              " 'jan': 87,\n",
              " 'period': 88,\n",
              " 'after': 89,\n",
              " 'till': 90,\n",
              " '21st': 91,\n",
              " 'that': 92,\n",
              " 'about': 93,\n",
              " 'follows': 94,\n",
              " 'model': 95,\n",
              " 'syllabus': 96,\n",
              " 'python': 97,\n",
              " 'ml': 98,\n",
              " 'studies': 99,\n",
              " 'learnwith': 100,\n",
              " 'deep': 101,\n",
              " 'nlp': 102,\n",
              " 'no': 103,\n",
              " 'miss': 104,\n",
              " 'live': 105,\n",
              " 'recording': 106,\n",
              " 'even': 107,\n",
              " 'class': 108,\n",
              " 'time': 109,\n",
              " '2': 110,\n",
              " 'how': 111,\n",
              " 'absolutely': 112,\n",
              " 'middle': 113,\n",
              " 'anytime': 114,\n",
              " 'submit': 115,\n",
              " 'with': 116,\n",
              " 'gmail': 117,\n",
              " 'registration': 118,\n",
              " 'related': 119,\n",
              " 'link': 120,\n",
              " 'entire': 121,\n",
              " 'suppose': 122,\n",
              " 'again': 123,\n",
              " 'days': 124,\n",
              " 'day': 125,\n",
              " 'refund': 126,\n",
              " 'living': 127,\n",
              " 'outside': 128,\n",
              " 'india': 129,\n",
              " 'should': 130,\n",
              " 'sending': 131,\n",
              " 'queries': 132,\n",
              " 'videos': 133,\n",
              " 'read': 134,\n",
              " 'but': 135,\n",
              " 'provided': 136,\n",
              " 'form': 137,\n",
              " '1': 138,\n",
              " 'clearance': 139,\n",
              " 'week': 140,\n",
              " 'just': 141,\n",
              " 'certificate': 142,\n",
              " 'earlier': 143,\n",
              " 'comes': 144,\n",
              " 'under': 145,\n",
              " 'guarantee': 146,\n",
              " 'dsmp': 147,\n",
              " '2023': 148,\n",
              " 'becomes': 149,\n",
              " 'approx': 150,\n",
              " 'covering': 151,\n",
              " 'following': 152,\n",
              " 'modules': 153,\n",
              " 'fundamentals': 154,\n",
              " 'libraries': 155,\n",
              " 'analysis': 156,\n",
              " 'sql': 157,\n",
              " 'maths': 158,\n",
              " 'machine': 159,\n",
              " 'algorithms': 160,\n",
              " 'practical': 161,\n",
              " 'mlops': 162,\n",
              " 'check': 163,\n",
              " 'detailed': 164,\n",
              " 'courses': 165,\n",
              " '637339afe4b0615a1bbed390': 166,\n",
              " 'both': 167,\n",
              " 'program’s': 168,\n",
              " 'curriculum': 169,\n",
              " 'recorded': 170,\n",
              " 'go': 171,\n",
              " 'back': 172,\n",
              " 'find': 173,\n",
              " 'schedule': 174,\n",
              " 'checkout': 175,\n",
              " 'sheet': 176,\n",
              " 'table': 177,\n",
              " 'docs': 178,\n",
              " 'spreadsheets': 179,\n",
              " 'd': 180,\n",
              " '16ootax': 181,\n",
              " 'a6oraecg4emgexhqqpv3noqpyku7rj6arozk': 182,\n",
              " 'edit': 183,\n",
              " 'usp': 184,\n",
              " 'sharing': 185,\n",
              " 'roughly': 186,\n",
              " 'last': 187,\n",
              " 'hours': 188,\n",
              " 'language': 189,\n",
              " 'spoken': 190,\n",
              " 'instructor': 191,\n",
              " 'during': 192,\n",
              " 'hinglish': 193,\n",
              " 'informed': 194,\n",
              " 'upcoming': 195,\n",
              " 'side': 196,\n",
              " 'before': 197,\n",
              " 'every': 198,\n",
              " 'become': 199,\n",
              " 'user': 200,\n",
              " 'non': 201,\n",
              " 'tech': 202,\n",
              " 'background': 203,\n",
              " 'lectures': 204,\n",
              " 'content': 205,\n",
              " 'provide': 206,\n",
              " 'solutions': 207,\n",
              " 'self': 208,\n",
              " 'evaluate': 209,\n",
              " 'yourself': 210,\n",
              " 'questions': 211,\n",
              " 'youtube': 212,\n",
              " 'channel': 213,\n",
              " 'amount': 214,\n",
              " 'unfortunately': 215,\n",
              " 'then': 216,\n",
              " '1st': 217,\n",
              " '30': 218,\n",
              " 'essentially': 219,\n",
              " 'wait': 220,\n",
              " 'end': 221,\n",
              " 'like': 222,\n",
              " 'making': 223,\n",
              " 'policy': 224,\n",
              " 'made': 225,\n",
              " 'post': 226,\n",
              " 'when': 227,\n",
              " 'view': 228,\n",
              " 'one': 229,\n",
              " 'tricky': 230,\n",
              " 'carefully': 231,\n",
              " 'valid': 232,\n",
              " 'purchased': 233,\n",
              " '20th': 234,\n",
              " 'purchase': 235,\n",
              " 'over': 236,\n",
              " 'installments': 237,\n",
              " 'aug': 238,\n",
              " '2024': 239,\n",
              " 'why': 240,\n",
              " 'lifetime': 241,\n",
              " 'because': 242,\n",
              " 'low': 243,\n",
              " 'reach': 244,\n",
              " 'out': 245,\n",
              " 'fill': 246,\n",
              " 'team': 247,\n",
              " 'still': 248,\n",
              " 'ask': 249,\n",
              " 'doubts': 250,\n",
              " 'select': 251,\n",
              " 'gmai': 252,\n",
              " 'criteria': 253,\n",
              " 'there': 254,\n",
              " 'criterias': 255,\n",
              " 'attempt': 256,\n",
              " 'assessments': 257,\n",
              " 'joining': 258,\n",
              " 'current': 259,\n",
              " 'clarify': 260,\n",
              " 'does': 261,\n",
              " 'mean': 262,\n",
              " 'dont': 263,\n",
              " 'any': 264,\n",
              " 'jobs': 265,\n",
              " 'matter': 266,\n",
              " 'interview': 267,\n",
              " 'calls': 268,\n",
              " 'planning': 269,\n",
              " 'placements': 270,\n",
              " 'afraid': 271,\n",
              " 'disappointed': 272,\n",
              " 'portfolio': 273,\n",
              " 'building': 274,\n",
              " 'soft': 275,\n",
              " 'skill': 276,\n",
              " 'industry': 277,\n",
              " 'mentors': 278,\n",
              " 'discussion': 279,\n",
              " 'job': 280,\n",
              " 'hunting': 281,\n",
              " 'strategies': 282}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1, len(tokenized_sentence)):\n",
        "    n_gram = tokenized_sentence[:i+1]\n",
        "    input_sequence.append(n_gram)"
      ],
      "metadata": {
        "id": "NY731Fi0BEMe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnnCS9SbBgVf",
        "outputId": "0397b162-b233-4871-e2ff-6a77db39573c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[93, 1], [93, 1, 13], [11, 7], [11, 7, 1], [11, 7, 1, 12]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the above list has different length, we will pad it to have similar length"
      ],
      "metadata": {
        "id": "1sP4RYemDQJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(x) for x in input_sequence)"
      ],
      "metadata": {
        "id": "5t3rWyq8DBnr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "gGnn6DghDb2u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences = pad_sequences(input_sequence, maxlen = max_length, padding = 'pre')"
      ],
      "metadata": {
        "id": "VfFdCENlDmRm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:, :-1]\n",
        "Y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "m74vOTO-DvBl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DohdAQ4tEMVq",
        "outputId": "6a0e9c9b-457d-4c03-81af-f171d99fbe07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(863, 56) (863,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since its a multi-classification problem, we want to get the probability of the most possible word which will come next in the sentence. In order to do so, we will look into our vocabulary and use on-hot encoding on it"
      ],
      "metadata": {
        "id": "6RNC8a9PFgbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "ITeCcQriFVid"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(tokenizer.word_index)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrLm3vHDGDp6",
        "outputId": "34b871e9-54f4-4a40-a0c8-362d2f9f86e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = to_categorical(Y, num_classes + 1) # plus one Tokenizer indices start from 1\\,\n",
        "# but one-hot encoding arrays are zero-indexed. Adding 1 to num_classes ensures all indices from 0 to the maximum index are covered."
      ],
      "metadata": {
        "id": "CMwnz-tGF70R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZtYDKl6Gps3",
        "outputId": "fd2011c5-ccf9-4198-ebf3-426fd2904fd1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(863, 283)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "We use the embedding layer to convert the sparse matrix (X) into dense matrix.  The Embedding layer transforms integer-encoded words into dense, meaningful vectors that capture semantic relationships, reducing dimensionality and improving the efficiency and effectiveness of NLP models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6TEJNyIcIQzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "UdVVaCkTHcHZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 100: The dimension of the dense embedding. Each word will be represented as a 100-dimensional vector. input_length=56: Each input sequence has 56 tokens.\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(283, 100, input_length = 56),\n",
        "    LSTM(150), # number of neurons\n",
        "    Dense(283, activation = 'softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "oHBr5ECYHxn7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "zX7606DmI3LE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg6CFnlUJCKZ",
        "outputId": "e641eb4a-d1e1-4948-a412-7ebc66a8f019"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 56, 100)           28300     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 283)               42733     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 221633 (865.75 KB)\n",
            "Trainable params: 221633 (865.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs = 500, verbose = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_aeGwIqJHrG",
        "outputId": "d644579f-195c-4318-a031-a97d7f0aaae6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7adc92123310>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "mV6orgCdNXK6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the model"
      ],
      "metadata": {
        "id": "hGxFS--bMeYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'mail'\n",
        "\n",
        "## tokenize\n",
        "token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "## padding\n",
        "padded_token = pad_sequences([token_text], maxlen = 56, padding = 'pre')\n",
        "\n",
        "## predict will give a list of 283 shape, probability for each word which can come next in the sentence\n",
        "position = np.argmax(model.predict(padded_token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edIktd4SMckt",
        "outputId": "f10776d5-abbd-47e6-9c56-4c81e20d1306"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1jaYWUQNrql",
        "outputId": "b4fda5aa-4281-4e38-d488-d971d7e9e1dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "  if index == position:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5urM1QKM-7j",
        "outputId": "88c4ce92-a15c-4440-b5c9-6f0ecdfb9cdf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing the whole sentence based on a word"
      ],
      "metadata": {
        "id": "6Q309BReN47j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'mail'\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "  padded_token = pad_sequences([token_text], maxlen = 56, padding = 'pre')\n",
        "\n",
        "  position = np.argmax(model.predict(padded_token))\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == position:\n",
        "      text = text + ' ' + word\n",
        "      print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7oWqLkvNve5",
        "outputId": "d5a3a85b-1836-4413-b2d5-77825b4ab901"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "mail this\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google sheet\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google sheet to\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google sheet to see\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "mail this google sheet to see month\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google sheet to see month by\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "mail this google sheet to see month by month\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "mail this google sheet to see month by month time\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "mail this google sheet to see month by month time table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP7ecOsOPMDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}